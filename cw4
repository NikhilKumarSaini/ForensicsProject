import os
import numpy as np
from PIL import Image
import cv2


def compute_ela_score(forensic_output_dir: str) -> float:
    """
    ELA-based manipulation severity score.import os
import numpy as np
from PIL import Image
import cv2


def compute_ela_score(forensic_output_dir: str) -> float:
    """
    ELA-based manipulation severity score.

    Output:
    - 0.0  → clean
    - >0.0 → severity (0–1)

    Detects BOTH:
    1. Localized ELA inconsistencies
    2. Distributed but abnormally strong ELA patterns
    """

    ela_dir = os.path.join(forensic_output_dir, "ELA")
    if not os.path.exists(ela_dir):
        return 0.0

    page_scores = []

    for fname in os.listdir(ela_dir):
        if not fname.lower().endswith(".jpg"):
            continue

        path = os.path.join(ela_dir, fname)

        # ===============================
        # LOAD ELA IMAGE
        # ===============================
        try:
            gray = np.array(
                Image.open(path).convert("L"),
                dtype=np.float32
            ) / 255.0
        except Exception:
            continue

        h, w = gray.shape
        total_pixels = h * w

        # ===============================
        # HARD CLEAN FILTER
        # ===============================
        if np.max(gray) < 0.05:
            page_scores.append(0.0)
            continue

        if np.mean(gray > 0.05) < 0.001:
            page_scores.append(0.0)
            continue

        # ===============================
        # STEP 1 — ACTIVE ELA PIXELS
        # ===============================
        active_mask = gray > 0.04
        active_count = int(np.sum(active_mask))

        if active_count < 120:
            page_scores.append(0.0)
            continue

        active_vals = gray[active_mask]

        # ===============================
        # STEP 2 — STRONG ELA RESPONSES
        # ===============================
        high_thresh = np.percentile(active_vals, 92)
        high_mask = gray >= high_thresh
        high_count = int(np.sum(high_mask))

        # =====================================================
        # MODE A: LOCALIZED ELA (classic tampering)
        # =====================================================
        mask_uint8 = (high_mask * 255).astype(np.uint8)
        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(
            mask_uint8, connectivity=8
        )

        regions = []

        for i in range(1, num_labels):
            area = int(stats[i, cv2.CC_STAT_AREA])
            x, y, bw, bh, _ = stats[i]

            if area < 100:
                continue
            if bw < 6 or bh < 6:
                continue

            regions.append((area, x, y, bw, bh))

        # ===============================
        # CLEAN TEXT / SCAN NOISE FILTER
        # ===============================
        if regions:
            ys = [r[2] for r in regions]
            areas = [r[0] for r in regions]

            y_std = np.std(ys)
            area_std = np.std(areas) / (np.mean(areas) + 1e-6)

            # Benign text-line noise only if small & uniform
            if y_std < 12 and area_std < 0.35 and len(regions) < 6:
                regions = []  # suppress as clean

        # =====================================================
        # MODE B: DISTRIBUTED STRONG ELA (document regeneration)
        # =====================================================
        distributed_flag = False
        if not regions:
            # Strong ELA everywhere but no clear blobs
            if high_count > 0.01 * total_pixels and np.mean(active_vals) > 0.12:
                distributed_flag = True

        # =====================================================
        # FINAL CLEAN DECISION
        # =====================================================
        if not regions and not distributed_flag:
            page_scores.append(0.0)
            continue

        # ===============================
        # SEVERITY SCORING
        # ===============================

        # 1️⃣ ELA energy
        energy = float(np.mean(gray[high_mask]))

        # 2️⃣ Area involvement
        concentration = high_count / total_pixels

        # 3️⃣ Location count
        if regions:
            location_score = min(1.0, len(regions) / 4.0)
            largest_patch_ratio = max(r[0] for r in regions) / total_pixels
        else:
            # Distributed ELA → low location confidence
            location_score = 0.3
            largest_patch_ratio = concentration

        raw_score = (
            0.40 * energy +
            0.25 * concentration * 10.0 +
            0.20 * largest_patch_ratio * 15.0 +
            0.15 * location_score
        )

        page_scores.append(min(1.0, raw_score))

    if not page_scores:
        return 0.0

    return float(round(max(page_scores), 3))


    Output semantics:
    - 0.0  → clean / no meaningful ELA inconsistency
    - >0.0 → severity of manipulation (0–1)

    Design:
    - Low-content & flat-page guards
    - Patch-level ELA energy analysis
    - Location-aware severity scoring
    - Penalizes multiple inconsistent regions
    """

    ela_dir = os.path.join(forensic_output_dir, "ELA")
    if not os.path.exists(ela_dir):
        return 0.0

    page_scores = []

    for fname in os.listdir(ela_dir):
        if not fname.lower().endswith(".jpg"):
            continue

        img_path = os.path.join(ela_dir, fname)

        # ===============================
        # LOAD IMAGE
        # ===============================
        try:
            gray = np.array(
                Image.open(img_path).convert("L"),
                dtype=np.float32
            ) / 255.0
        except Exception:
            continue

        h, w = gray.shape
        total_pixels = h * w

        # ===============================
        # HARD CLEAN FILTERS
        # ===============================

        # Flat / empty ELA image
        if np.max(gray) < 0.05:
            page_scores.append(0.0)
            continue

        # Almost no ELA activity
        if np.mean(gray > 0.05) < 0.001:
            page_scores.append(0.0)
            continue

        # ===============================
        # STEP 1 — ACTIVE ELA RESIDUALS
        # ===============================
        active_mask = gray > 0.05
        active_count = int(np.sum(active_mask))

        if active_count < 150:
            page_scores.append(0.0)
            continue

        active_vals = gray[active_mask]

        # ===============================
        # STEP 2 — STRONG ELA RESPONSES
        # ===============================
        high_thresh = np.percentile(active_vals, 95)
        high_mask = gray >= high_thresh
        high_count = int(np.sum(high_mask))

        if high_count < 80:
            page_scores.append(0.0)
            continue

        # ===============================
        # STEP 3 — CONNECTED COMPONENTS
        # ===============================
        mask_uint8 = (high_mask * 255).astype(np.uint8)
        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(
            mask_uint8, connectivity=8
        )

        regions = []

        for i in range(1, num_labels):
            area = int(stats[i, cv2.CC_STAT_AREA])
            x, y, bw, bh, _ = stats[i]

            if area < 120:
                continue
            if bw < 6 or bh < 6:
                continue

            regions.append((area, x, y, bw, bh))

        # ===============================
        # CLEAN TEXT / SCAN NOISE FILTER
        # ===============================
        if regions:
            ys = [r[2] for r in regions]
            areas = [r[0] for r in regions]

            y_std = np.std(ys)
            area_std = np.std(areas) / (np.mean(areas) + 1e-6)

            # Uniform scan noise → clean
            if y_std < 12 and area_std < 0.35 and len(regions) < 8:
                page_scores.append(0.0)
                continue

        if not regions:
            page_scores.append(0.0)
            continue

        # ===============================
        # SEVERITY SCORING
        # ===============================

        # 1️⃣ ELA energy
        energy = float(np.mean(gray[high_mask]))

        # 2️⃣ Area involvement
        concentration = high_count / total_pixels
        area_ratio = concentration

        # 3️⃣ Largest inconsistent block
        largest_patch_ratio = max(r[0] for r in regions) / total_pixels

        # 4️⃣ Number of inconsistent locations
        location_count = len(regions)
        location_score = min(1.0, location_count / 4.0)

        # ===============================
        # FINAL WEIGHTED SCORE
        # ===============================
        raw_score = (
            0.35 * energy +
            0.20 * concentration * 10.0 +
            0.15 * area_ratio * 10.0 +
            0.15 * largest_patch_ratio * 15.0 +
            0.15 * location_score
        )

        page_scores.append(min(1.0, raw_score))

    if not page_scores:
        return 0.0

    return float(round(max(page_scores), 3))
